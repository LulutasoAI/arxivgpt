However, due to its high computational requirements, some other consensus algorithms like Proof-of- Stake [2], PBFT [3], and Proof-of-Authority [4] were proposed and accepted by many blockchain networks and cryptocurren- cies.Blockchain consensus algorithms constitute a focal point of extensive research, and numerous comparative analyses have contributed to a nuanced understanding of this intricate field [9], [11].The latter study, conducted by Saleh et al. [2], delves deeply into the energy-efficient and secure attributes inherent in PoS, shedding light on its potential advantages over traditional consensus methods.Importantly, none of the existing works, to date, has specifically focused on the crucial aspects of malicious node detection and dynamic adaptation of consen- sus mechanisms to changing user behaviors.Some agents may derive unfair advantages from relatively high computing powers, potentially gaining unexpected control over the consensus by processing the maximum number of transactions.These criteria serve as indispensable benchmarks for all participating agents in the voting process, fortifying the consensus mechanism against malicious activities and enhancing the overall resilience of the blockchain network.The primary objective of this research is to formulate and execute a distributed consensus algorithm that embodies fairness, resilience against malicious activities, and adapt- ability to dynamic conditions.Liu, and A. Nallanathan, “Transmit power pool design for grant-free noma-iot networks via deep reinforcement learn- ing,” IEEE Transactions on Wireless Communications , vol.To achieve this without human supervision, the agent initialized in a training environment asks an off-the-shelf LLM to generate useful task instructions given the de- scription of the available objects in the current en- vironment d(s0).Each environment setup is designed to evaluate a basic skill in Minecraft game, such as harvesting logs or combating monsters, thus necessary objects or animals are placed near the initial position or a necessary tool is provided to the agent.For the task proposal, as described in Section 3.1, we prompt an LLM with the initial textual information of nearby objects, entities, and the current tool in the agent’s hand retrieved from the game simulator state.While MineDojo proposes a set of new tasks brainstormed with LLM and to reward agents using VLM trained on the YouTube knowledge base, how to integrate and ground them in an open-ended learning environment is yet explored.[6]Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choro- manski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, Pete Florence, Chuyuan Fu, Montse Gonzalez Arenas, Keerthana Gopalakrishnan, Kehang Han, Karol Hausman, Alex Herzog, Jasmine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu, Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kanishka Rao, Krista Reymann, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait Singh, Radu Soricut, Huong Tran, Vincent Vanhoucke, Quan Vuong, Ayzaan Wahid, Stefan Welker, Paul Wohlhart, Jialin Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich.[7]Yevgen Chebotar, Quan Vuong, Karol Hausman, Fei Xia, Yao Lu, Alex Irpan, Aviral Kumar, Tianhe Yu, Alexander Herzog, Karl Pertsch, Keerthana Gopalakrishnan, Julian Ibarz, Ofir Nachum, Sumedh Anand Sontakke, Grecia Salazar, Huong T Tran, Jodilyn Peralta, Clayton Tan, Deeksha Manjunath, Jaspiar Singh, Brianna Zitkovich, Tomas Jackson, Kanishka Rao, Chelsea Finn, and Sergey Levine.[21] Alejandro Escontrela, Ademi Adeniji, Wilson Yan, Ajay Jain, Xue Bin Peng, Ken Goldberg, Youngwoon Lee, Danijar Hafner, and Pieter Abbeel.10[42] Anurag Ajay, Seungwook Han, Yilun Du, Shaung Li, Abhi Gupta, Tommi Jaakkola, Josh Tenenbaum, Leslie Kaelbling, Akash Srivastava, and Pulkit Agrawal.To train the agents’ policy, existing work uses local, egoistic, per-agent rewards, which can lead to sub-optimal behavior from the perspective of the central operator interested in maximizing the system-wide profit.The core of our method- ology is a new advantage function, which estimates the individual agents’ contribution to the global reward, as such aligning their goal with the central operator, ultimately following a system-optimal policy.Further- more, most previous work considers non-autonomous MoD, aiming to maximize revenues (e.g., Wang et al., 2018; Xu et al., 2018; Tang et al., 2019), while AMoD should focus on profit maxi- mization (cf.A crucial challenge in our setting is deriving per-agent contributions to global success from a shared reward signal, i.e., a credit assignment problem (Weiß, 1995; Wolpert and Tumer, 1999; Chang et al., 2003).Solution approaches like inverse reinforcement learning (e.g., Ng and Rus- sell, 2000; Hadfield-Menell et al., 2017; Lin et al., 2018) or value decomposition (e.g., Kok and Vlassis, 2006; Sunehag et al., 2018; Son et al., 2019; Rashid et al., 2020) are not applicable to our setting, because we cannot observe the behavior of an optimal agent and do not use Q-learning.Contribution To close the research gap outlined above, we develop the first scalable MADRL-based algorithm for vehicle dispatching in profit-maximizing AMoD that trains agents via global rewards.Reward scheduling Usually, one could resolve the scalability problem of COMAadjstraightforwardly by adjusting the critic to accommodate value factorization (e.g., Su et al., 2021), but this approach is infeasible in our setting as the number of agents is variable.Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki, Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z. Leibo, Karl Tuyls, and Thore Graepel.1 Introduction Recently, Deep Reinforcement Learning (DRL) has achieved breakthrough success in various application scenarios, including video games (Mnih et al. 2015), recom- mender systems (Afsar, Crump, and Far 2022), and robotics control (Lee et al. 2020).These achievements typically rely on the Deep Neural Networks (DNNs) as function approximators for their strong expressive power, which enables the end-to-end learning of policies in complex environments with high-dimension state spaces, such as images observations (Hornik, Stinchcombe, and White 1989; Mnih et al. 2015; Kaiser et al. 2020).However, DNNs typically lack robustness due to their highly non-linear and black-box nature, resulting in unrea- sonable and unpredictable outputs when inputs are perturbed slightly (Madry et al. 2018; Yuan et al. 2019).Despite the excellent robustness, these methods require training extra attackers or finding adversaries for the observations, which incurs additional computational and sampling costs, thereby limiting their practical applications.Additionally, we design a training framework forSortRL based on Policy Distillation (Rusu et al. 2016), which enables the agent to solve the given tasks success- fully while addressing robustness requirements against ob- servation perturbations.For instance, they may suffer from high computational costs (Zhang et al. 2021b) and struggle to cope with strong perturbations, such as pertur- bations strength greater than 5/255in video games (Wu and V orobeychik 2022).Recently, several Lipschitz Neural Networks (LNN) have been proposed to enhance robustness, including Spectral Norm (Gouk et al. 2021), GroupSort (Anil, Lucas, and Grosse 2019), and l∞- distance neuron (Zhang et al. 2022a, 2021a).As shown in the tables, SortRL outperforms baseline methods and achieves higher episode rewards on video game tasks with different perturbation strength, demonstrating the effectiveness of our approach.From Centralized to Self-Supervised: Pursuing Realistic Multi-Agent Reinforcement Learning Violet Xiang∗Logan Cross Jan-Philipp Fränken Nick Haber Stanford University Abstract In real-world environments, autonomous agents rely on their egocentric obser- vations.∗Corresponding author: ziyxiang@stanford.edu 2nd Workshop on Agent Learning in Open-Endedness (ALOE) at NeurIPS 2023.arXiv:2312.08662v1  [cs.MA]  14 Dec 2023To make optimal decisions, they must discern the intentions of others and group dynamics based solely on this observable information.[Ndousse et al., 2021] show that integrating an model-based auxiliary loss, allowing agents to predict upcoming states from observations, promotes social learning in certain environment even without intrinsic motivation.For social influence agents we follow the setup in the original paper [Jaques et al., 2019] by sharing the same CNN encoder as the policy and adding a separate head with two MLP layers and a GRU as the MOA model.Edward Hughes, Joel Z Leibo, Matthew Phillips, Karl Tuyls, Edgar Dueñez-Guzman, Antonio García Castañeda, Iain Dunning, Tina Zhu, Kevin McKee, Raphael Koster, et al. Inequity aversion improves cooperation in intertemporal social dilemmas.Kevin R McKee, Edward Hughes, Tina O Zhu, Martin J Chadwick, Raphael Koster, Antonio Garcia Castaneda, Charlie Beattie, Thore Graepel, Matt Botvinick, and Joel Z Leibo.Natasha Jaques, Angeliki Lazaridou, Edward Hughes, Caglar Gulcehre, Pedro Ortega, DJ Strouse, Joel Z Leibo, and Nando De Freitas.Joel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets, John P Agapiou, Peter Sunehag, Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel.All rights reserved.However, similar to other fields within machine learning, en- suring a high level of scientific rigour and sound experimen- tal methodology has become difficult as algorithms become more complex and computational requirements to perform experiments more extreme.And as we move away from syn- thetic problems like video games towards tasks with real world repercussions, it is important to ensure that trust in the field as a whole is not eroded over time by limited explain- ability of our results.Although the original analysis of (Gorsane et al. 2022) highlighted many issues, there was evidence of a more thorough evaluation beginning to gain traction in more recent years, with an increase in the number of ablation studies and an upward trend in the use of uncertainty quantification.Value decomposi- tion network (VDN) (Sunehag et al. 2017) introduced a new paradigm where a centralised critic is used to split the joint signal into individual agent rewards for better credit assign- ment.Additionally, aggregation met- rics are not present in 20% of recent publications and with the computational complexity of modern MARL it is very diffi- cult to evaluate over enough seeds to account for the variance in performance over different runs (Agarwal et al. 2022).Given these findings it may be more effective to make use of explanability frameworks like ShinRL (Kitamura and Yone- tani 2021) to determine a human interpreble representation of the learnt policy and from this compare the viability and variabilty of different methods.However, although SMAC-v2 does resolve the the limita- tions of SMAC-v1 w.r.t generalisation and overfitting, it can still be computationally expensive to conform to the required number of runs outlined in works like (Agarwal et al. 2022; Gorsane et al. 2022) as it relies on a full fledged video game as a back end.Bakhtin, A.; Brown, N.; Dinan, E.; Farina, G.; Flaherty, C.; Fried, D.; Goff, A.; Gray, J.; Hu, H.; Jacob, A. P.; Komeili, M.; Konath, K.; Kwon, M.; Lerer, A.; Lewis, M.; Miller, A. H.; Mitts, S.; Renduchintala, A.; Roller, S.; Rowe, D.; Shi, W.; Spisak, J.; Wei, A.; Wu, D. J.; Zhang, H.; and Zijlstra, M. 2022.In our framework, we design a low-latency reputation-based proof-of-stake (RPoS) consensus protocol to select highly reliable blockchain-enabled BSs to securely store MEC user requests and prevent data tampering attacks.SECURITY is a major concern in mobile edge computing (MEC) service provisioning given the prevalence of data tampering attacks leading to disruptive denial-of-service (DoS) in decentralized wireless networks [1, 2].In our model, an MEC service provider employs NBBSs with overlapping coverage to satisfy the dynamic time-varying requests for computation resources from multiple users with DoS probability constraints for the BSs.2 shows the decentralized architecture of our BC-DRL solution, which includes three entities: 1) Users, 2) MEC service provider managing the RPoS consensus, and 3) BSs implementing the DRL-based resource allocation algorithm.Then, to improve the resource utiliza- tion efficiency without jeopardizing the system security level, trusted committee BSs with high reputations are selected to manage the blockchain network.This equation reveals that the MEC service provisioning with blockchain has a very high tampering attack-resistant ability, as an attacker would need to compromise at least half of the BSs in the network to succeed time slot ~~Fig.In contrast, the proposed RPoS consensus exhibits robust resistance to tampering attacks, comparable to the security levels achieved by PoW, while significantly reducing computation resource requirements.Li, M. Tao, and W. Zhang, “Mobile com- munications, computing, and caching resources allocation for diverse services via multi-objective proximal policy optimization,” IEEE Trans.